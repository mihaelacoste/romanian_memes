import praw
import datetime
import time
import pandas as pd
import os

# --- Reddit API Credentials ---
# IMPORTANT: Replace with your actual credentials.
CLIENT_ID = "your_client_id_here"
CLIENT_SECRET = "your_client_secret_here"
USER_AGENT = "your_reddit_useragent" # Replace with your Reddit username
USERNAME = "your_reddit_username" # Your Reddit username
PASSWORD = "your_reddit_pass" # Your Reddit password

# --- Configuration ---
INPUT_POSTS_CSV_FILE = "romemes_posts.csv" # The CSV file with your scraped post IDs
OUTPUT_COMMENTS_CSV_FILE = "romemes_comments_from_csv.csv"
OUTPUT_COMMENTS_EXCEL_FILE = "romemes_comments_from_csv.xlsx"

# How many comments to fetch per post (approximate for top-level + some replies)
# Setting to None will fetch as many as possible within Reddit's API limits.
# Be careful with very active threads, can be many requests.
COMMENTS_LIMIT_PER_POST = 20 

def get_comments_for_posts_from_csv(input_csv_path, comments_limit_per_post=None):
    """
    Reads post IDs from a CSV file, fetches their comments using PRAW,
    and returns a list of comment data.
    """
    if not os.path.exists(input_csv_path):
        print(f"Error: Input posts CSV file '{input_csv_path}' not found.")
        return []

    try:
        df_posts = pd.read_csv(input_csv_path)
        # Assuming your 'id' column in the CSV is the 'unique_post_id'
        # and the original Reddit submission ID is part of it after the underscore.
        # We need the original Reddit submission ID to fetch comments.
        df_posts['original_submission_id'] = df_posts['id'].apply(lambda x: x.split('_')[-1])
        
        print(f"Successfully loaded {len(df_posts)} posts from {input_csv_path}.")
    except Exception as e:
        print(f"Error reading input posts CSV file '{input_csv_path}': {e}")
        return []

    try:
        reddit = praw.Reddit(
            client_id=CLIENT_ID,
            client_secret=CLIENT_SECRET,
            user_agent=USER_AGENT,
            username=USERNAME,
            password=PASSWORD
        )
        print(f"Successfully connected to Reddit as {reddit.user.me()} (read_only: {reddit.read_only})")
    except Exception as e:
        print(f"Error connecting to Reddit: {e}")
        print("Please check your API credentials and ensure your Reddit account is active.")
        return []

    all_comments_data = []

    print("\nFetching comments for each post from the CSV file...")
    # Iterate through each post record from the CSV
    for index, row in df_posts.iterrows():
        unique_post_id = row['id'] # Your custom unique ID
        original_reddit_submission_id = row['original_submission_id']
        post_title = row['title'] # For display purposes

        print(f"  Fetching comments for post: '{post_title}' (Original Reddit ID: {original_reddit_submission_id})")
        try:
            # Get the Submission object directly using its Reddit ID
            submission = reddit.submission(id=original_reddit_submission_id)
            
            # Replace 'MoreComments' objects with actual comments
            submission.comments.replace_more(limit=comments_limit_per_post) 
            
            # Iterate through all comments (top-level and replies)
            for comment in submission.comments.list():
                # Ensure comment is not a MoreComments object (though replace_more should handle this)
                if isinstance(comment, praw.models.MoreComments):
                    continue 
                
                comment_created_utc = datetime.datetime.fromtimestamp(comment.created_utc, tz=datetime.timezone.utc)
                
                all_comments_data.append({
                    "post_id": unique_post_id, # Link back to the post using your custom ID
                    "comment_id": comment.id,
                    "comment_parent_id": comment.parent_id, # Can be a post ID (t3_) or another comment ID (t1_)
                    "comment_author": comment.author.name if comment.author else "[deleted]",
                    "comment_score": comment.score,
                    "comment_body": comment.body,
                    "comment_created_utc": comment_created_utc.strftime('%Y-%m-%d %H:%M:%S UTC'),
                    "comment_permalink": f"https://reddit.com{comment.permalink}"
                })
            print(f"    Fetched {len(submission.comments.list())} comments for this post.") # Note: this is after replace_more
            
        except praw.exceptions.PRAWException as pe:
            print(f"    PRAW Error fetching comments for post {original_reddit_submission_id}: {pe}")
        except Exception as e:
            print(f"    An unexpected error occurred for post {original_reddit_submission_id}: {e}")
        
        time.sleep(0.5) # Small delay between fetching comments for different posts
            
    return all_comments_data

if __name__ == "__main__":
    comments_data = get_comments_for_posts_from_csv(INPUT_POSTS_CSV_FILE, COMMENTS_LIMIT_PER_POST)

    if comments_data:
        print(f"\nFound a total of {len(comments_data)} comments. Creating CSV and Excel files for comments...")
        df_comments = pd.DataFrame(comments_data)
        
        output_df_comments = df_comments[[
            "post_id", # Link to the post
            "comment_id",
            "comment_parent_id",
            "comment_author",
            "comment_score",
            "comment_body",
            "comment_created_utc",
            "comment_permalink"
        ]]

        output_df_comments.to_csv(OUTPUT_COMMENTS_CSV_FILE, index=False, encoding='utf-8')
        print(f"Comment data saved to {OUTPUT_COMMENTS_CSV_FILE}")

        output_df_comments.to_excel(OUTPUT_COMMENTS_EXCEL_FILE, index=False)
        print(f"Comment data saved to {OUTPUT_COMMENTS_EXCEL_FILE}")
    else:
        print("No comments found or no posts to process from the CSV file.")
